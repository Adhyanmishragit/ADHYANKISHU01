{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1vbVUpEbWuwN8Z8pXPgHAF9-53uLFYFZZ",
      "authorship_tag": "ABX9TyO6n6cSrldxyiEzNxvrCYYD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adhyanmishragit/ADHYANKISHU01/blob/main/Copy_of_Blackcoffer_assignment_solution_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blackcoffer\n",
        "Consulting Website:  https://blackcoffer.com | https://lsalead.com/\n",
        "Web App Products: https://netclan.com/ | https://insights.blackcoffer.com/ | https://hirekingdom.com/ | https://workcroft.com/\n",
        "Mobile App Products: Netclan | Bwstory\n",
        "\n",
        "Data Extraction and NLP\n",
        "Test Assignment\n",
        "\n",
        "Objective\n",
        "The objective of this assignment is to extract textual data articles from the given URL and perform text analysis to compute variables that are explained below.\n",
        "\n",
        "Data Extraction\n",
        "Input.xlsx\n",
        "For each of the articles, given in the input.xlsx file, extract the article text and save the extracted article in a text file with URL_ID as its file name.\n",
        "While extracting text, please make sure your program extracts only the article title and the article text. It should not extract the website header, footer, or anything other than the article text.\n",
        "\n",
        "NOTE: YOU MUST USE PYTHON PROGRAMMING TO EXTRACT DATA FROM THE URLs. YOU CAN USE BEATIFULSOUP, SELENIUM OR SCRAPY, OR ANY OTHER PYTHON LIBRARIES THAT YOU PREFER FOR DATA CRAWLING.\n",
        "\n",
        "Data Analysis\n",
        "For each of the extracted texts from the article, perform textual analysis and compute variables, given in the output structure excel file. You need to save the output in the exact order as given in the output structure file, “Output Data Structure.xlsx”\n",
        "NOTE: YOU MUST USE PYTHON PROGRAMMING FOR THE DATA ANALYSIS\n",
        "\n",
        "\n",
        "Variables\n",
        "Definition of each of the variables given in the “Text Analysis.docx” file.\n",
        "Look for these variables in the analysis document (Text Analysis.docx):\n",
        "POSITIVE SCORE\n",
        "NEGATIVE SCORE\n",
        "POLARITY SCORE\n",
        "SUBJECTIVITY SCORE\n",
        "AVG SENTENCE LENGTH\n",
        "PERCENTAGE OF COMPLEX WORDS\n",
        "FOG INDEX\n",
        "AVG NUMBER OF WORDS PER SENTENCE\n",
        "COMPLEX WORD COUNT\n",
        "WORD COUNT\n",
        "SYLLABLE PER WORD\n",
        "PERSONAL PRONOUNS\n",
        "AVG WORD LENGTH\n",
        "\n",
        "Output Data Structure\n",
        "Output Variables:\n",
        "All input variables in “Input.xlsx”\n",
        "POSITIVE SCORE\n",
        "NEGATIVE SCORE\n",
        "POLARITY SCORE\n",
        "SUBJECTIVITY SCORE\n",
        "AVG SENTENCE LENGTH\n",
        "PERCENTAGE OF COMPLEX WORDS\n",
        "FOG INDEX\n",
        "AVG NUMBER OF WORDS PER SENTENCE\n",
        "COMPLEX WORD COUNT\n",
        "WORD COUNT\n",
        "SYLLABLE PER WORD\n",
        "PERSONAL PRONOUNS\n",
        "AVG WORD LENGTH\n",
        "Checkout output data structure spreadsheet for the format of your output, i.e. “Output Data Structure.xlsx”.\n"
      ],
      "metadata": {
        "id": "kivdHlrYML3K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaa0uuepC7lM"
      },
      "outputs": [],
      "source": [
        "# Importing the required libraries\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract article text from a URL\n",
        "def extract_article_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        article = soup.find('article')\n",
        "        if article:\n",
        "            return article.get_text()\n",
        "        else:\n",
        "            return None\n",
        "    except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "BOjQxl0wI8SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate syllable count in a word\n",
        "def syllable_count(word):\n",
        "    word = word.lower()\n",
        "    if word == 'i':\n",
        "        return 1\n",
        "    count = 0\n",
        "    vowels = 'aeiouy'\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for index in range(1, len(word)):\n",
        "        if word[index] in vowels and word[index - 1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith('e'):\n",
        "        count -= 1\n",
        "    if count == 0:\n",
        "        count = 1\n",
        "    return count"
      ],
      "metadata": {
        "id": "ybVrvGyuJBl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count personal pronouns\n",
        "def count_personal_pronouns(text):\n",
        "    personal_pronouns = ['I', 'we', 'my', 'ours', 'us']\n",
        "    pattern = r'\\b(?:' + '|'.join(personal_pronouns) + r')\\b'\n",
        "    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
        "    return len(matches)\n"
      ],
      "metadata": {
        "id": "WlvJocfwJhuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Read input Excel file\n",
        "input_file = \"/content/Input.xlsx\"\n",
        "output_file = \"/content/OutputDataStructure.xlsx\"\n",
        "\n",
        "input_data = pd.read_excel(input_file)\n"
      ],
      "metadata": {
        "id": "BfKiwTymJ1cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store results\n",
        "output_data = []\n",
        "\n",
        "# Loop through each row in the input data\n",
        "for index, row in input_data.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    article_text = extract_article_text(url)\n",
        "\n",
        "    if article_text:\n",
        "        # Perform text analysis using TextBlob and other methods\n",
        "        blob = TextBlob(article_text)\n",
        "        words = blob.words\n",
        "\n",
        "        # Sentiment analysis\n",
        "        positive_score = blob.sentiment.polarity\n",
        "        negative_score = -blob.sentiment.polarity\n",
        "        polarity_score = blob.sentiment.polarity\n",
        "        subjectivity_score = (positive_score + negative_score) / (len(words) + 0.000001)\n",
        "\n",
        "        # Readability analysis\n",
        "        sentences = blob.sentences\n",
        "        avg_sentence_length = len(words) / len(sentences)\n",
        "        complex_words = [word for word in words if syllable_count(word) > 2]\n",
        "        percentage_complex_words = (len(complex_words) / len(words)) * 100\n",
        "        fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "        # Other analyses\n",
        "        avg_words_per_sentence = len(words) / len(sentences)\n",
        "        complex_word_count = len(complex_words)\n",
        "        total_words = len(words)\n",
        "        syllable_per_word = sum(syllable_count(word) for word in words) / total_words\n",
        "        personal_pronouns = count_personal_pronouns(article_text)\n",
        "        avg_word_length = sum(len(word) for word in words) / total_words\n",
        "\n",
        "        output_row = [url_id, positive_score, negative_score, polarity_score, subjectivity_score,\n",
        "                      avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence,\n",
        "                      complex_word_count, total_words, syllable_per_word, personal_pronouns, avg_word_length]\n",
        "\n",
        "        output_data.append(output_row)"
      ],
      "metadata": {
        "id": "AaoNrREWKL3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame for the output data\n",
        "output_columns = ['URL_ID', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE',\n",
        "                  'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE',\n",
        "                  'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n",
        "\n",
        "output_df = pd.DataFrame(output_data, columns=output_columns)\n",
        "\n",
        "# Save the output DataFrame to an Excel file\n",
        "output_df.to_excel(output_file, index=False)\n"
      ],
      "metadata": {
        "id": "GMG5J6GOKfQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fSsbzQX-MG_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "oc9EB8SYDc9z"
      }
    }
  ]
}