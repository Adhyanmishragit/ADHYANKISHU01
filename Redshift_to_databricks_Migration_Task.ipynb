{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba6f6dc8-711b-4435-8397-b6b2cd05fe57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "CREATE CATALOG IF NOT EXISTS dummy_catalog;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a257839-f318-423a-8461-262657608d10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS dummy_catalog.dummy_schema;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36971d0d-54cc-4e17-bcb0-4f20fcd8c927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS dummy_schema;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b18ee5ff-381a-49cd-b33e-90d3c45acfc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG dummy_catalog;\n",
    "USE SCHEMA dummy_schema;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c387a64-6cd3-4cec-b386-a3f0dedaf6a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create 'orders' table\n",
    "CREATE OR REPLACE TEMP VIEW orders AS\n",
    "SELECT * FROM VALUES\n",
    "  (101, 1, 1001, DATE('2025-06-01'), 650, 'shipped'),\n",
    "  (102, 2, 1002, DATE('2025-05-20'), 250, 'shipped'),\n",
    "  (103, 3, 1003, DATE('2025-05-10'), 80,  'pending'),\n",
    "  (104, 1, 1002, DATE('2025-06-10'), 120, 'shipped')\n",
    "AS orders(order_id, customer_id, product_id, order_date, total_amount, status);\n",
    "\n",
    "-- Create 'customers' table\n",
    "CREATE OR REPLACE TEMP VIEW customers AS\n",
    "SELECT * FROM VALUES\n",
    "  (1, 'Alice Johnson'),\n",
    "  (2, 'Bob Smith'),\n",
    "  (3, 'Charlie Davis')\n",
    "AS customers(customer_id, customer_name);\n",
    "\n",
    "-- Create 'products' table\n",
    "CREATE OR REPLACE TEMP VIEW products AS\n",
    "SELECT * FROM VALUES\n",
    "  (1001, 'Laptop'),\n",
    "  (1002, 'Headphones'),\n",
    "  (1003, 'Keyboard')\n",
    "AS products(product_id, product_name);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7176ea8-e5e5-4383-883f-deb5311f7cde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Converted Queries ===\nQuery 1:\nSELECT DISTINCT\n    o.order_id,\n    c.customer_name,\n    p.product_name,\n    o.order_date,\n    CASE \n        WHEN o.total_amount > 500 THEN 'High'\n        WHEN o.total_amount BETWEEN 100 AND 500 THEN 'Medium'\n        ELSE 'Low'\n    END AS order_value_category\nFROM\n    orders o\nJOIN\n    customers c ON o.customer_id = c.customer_id\nJOIN\n    products p ON o.product_id = p.product_id\nWHERE\n    o.order_date >= DATE_SUB(CURRENT_DATE, 30)\n    AND o.status = 'shipped'\nORDER BY\n    o.order_date DESC\n------------------------------------------------------------\n\n=== Conversion Summary ===\nTotal queries processed: 1\nSuccessfully converted: 1\nRequires manual review: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_func_name(name):\n",
    "    return re.sub(r'\\(.*\\)', '', str(name)).strip().upper()\n",
    "\n",
    "# 1. Load mapping from CSV (robust to encoding)\n",
    "csv_path = '/Volumes/workspace/default/adhyan/Redshift and Databricks functions(Sheet1).csv'\n",
    "try:\n",
    "    mapping_df = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
    "except UnicodeDecodeError:\n",
    "    mapping_df = pd.read_csv(csv_path, encoding='latin1')\n",
    "mapping_df.columns = [col.strip() for col in mapping_df.columns]\n",
    "\n",
    "# 2. Build direct and manual mappings\n",
    "direct_map = mapping_df[mapping_df['need to change'].str.contains('direct', case=False, na=False)]\n",
    "manual_map = mapping_df[mapping_df['need to change'].str.contains('manual|needs to be done manually|needs to do manually|needs to pass', case=False, na=False)]\n",
    "\n",
    "function_map = {\n",
    "    clean_func_name(row['Redshift_function']): clean_func_name(row['Databricks_function'])\n",
    "    for _, row in direct_map.iterrows()\n",
    "}\n",
    "manual_funcs = set([clean_func_name(f) for f in manual_map['Redshift_function']])\n",
    "\n",
    "# 3. Read SQL file\n",
    "sql_path = '/Volumes/workspace/default/adhyan/Redshift_conversion.sql'\n",
    "with open(sql_path, 'r', encoding='utf-8') as f:\n",
    "    sql_script = f.read()\n",
    "\n",
    "queries = [q.strip() for q in sql_script.split(';') if q.strip()]\n",
    "\n",
    "# 4. Function replacement logic\n",
    "def replace_functions(query, function_map, manual_funcs):\n",
    "    flagged = []\n",
    "    converted = query\n",
    "\n",
    "    # Replace direct mappings from CSV\n",
    "    for redshift_func, databricks_func in function_map.items():\n",
    "        pattern = re.compile(rf'\\b{re.escape(redshift_func)}\\s*\\(', re.IGNORECASE)\n",
    "        converted = pattern.sub(f'{databricks_func}(', converted)\n",
    "\n",
    "    # Flag manual functions from CSV\n",
    "    for func in manual_funcs:\n",
    "        if re.search(rf'\\b{re.escape(func)}\\s*\\(', converted, re.IGNORECASE):\n",
    "            flagged.append(func)\n",
    "\n",
    "    return converted, flagged\n",
    "\n",
    "# 5. Process all queries\n",
    "converted_queries = []\n",
    "manual_flags = []\n",
    "for q in queries:\n",
    "    converted, flagged = replace_functions(q, function_map, manual_funcs)\n",
    "    converted_queries.append(converted)\n",
    "    if flagged:\n",
    "        manual_flags.append((converted, flagged))\n",
    "\n",
    "# 6. Output results\n",
    "out_path = '/Volumes/workspace/default/adhyan/Redshift_conversion_converted.sql'\n",
    "with open(out_path, 'w', encoding='utf-8') as f:\n",
    "    for q in converted_queries:\n",
    "        f.write(q.strip() + \";\\n\\n\")\n",
    "\n",
    "# Print summary\n",
    "print(\"=== Converted Queries ===\")\n",
    "for i, q in enumerate(converted_queries, 1):\n",
    "    print(f\"Query {i}:\")\n",
    "    print(q)\n",
    "    print('-' * 60)\n",
    "\n",
    "if manual_flags:\n",
    "    print('\\n=== Queries Requiring Manual Intervention ===')\n",
    "    for i, (query, funcs) in enumerate(manual_flags, 1):\n",
    "        print(f\"Query {i} - Functions needing manual review: {', '.join(funcs)}\")\n",
    "        print(query)\n",
    "        print('-' * 60)\n",
    "\n",
    "print('\\n=== Conversion Summary ===')\n",
    "print(f'Total queries processed: {len(queries)}')\n",
    "print(f'Successfully converted: {len(queries) - len(manual_flags)}')\n",
    "print(f'Requires manual review: {len(manual_flags)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5d50d7a-1131-4a38-b39a-0d7ca919c052",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>customer_name</th><th>product_name</th><th>order_date</th><th>order_value_category</th></tr></thead><tbody><tr><td>104</td><td>Alice Johnson</td><td>Headphones</td><td>2025-06-10</td><td>Medium</td></tr><tr><td>101</td><td>Alice Johnson</td><td>Laptop</td><td>2025-06-01</td><td>High</td></tr><tr><td>102</td><td>Bob Smith</td><td>Headphones</td><td>2025-05-20</td><td>Medium</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         104,
         "Alice Johnson",
         "Headphones",
         "2025-06-10",
         "Medium"
        ],
        [
         101,
         "Alice Johnson",
         "Laptop",
         "2025-06-01",
         "High"
        ],
        [
         102,
         "Bob Smith",
         "Headphones",
         "2025-05-20",
         "Medium"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "order_id",
            "nullable": false,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "customer_name",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "product_name",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "order_date",
            "nullable": true,
            "type": "date"
           },
           {
            "metadata": {},
            "name": "order_value_category",
            "nullable": false,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 31
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "customer_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "order_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "order_value_category",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "SELECT DISTINCT\n",
    "    o.order_id,\n",
    "    c.customer_name,\n",
    "    p.product_name,\n",
    "    o.order_date,\n",
    "    CASE \n",
    "        WHEN o.total_amount > 500 THEN 'High'\n",
    "        WHEN o.total_amount BETWEEN 100 AND 500 THEN 'Medium'\n",
    "        ELSE 'Low'\n",
    "    END AS order_value_category\n",
    "FROM\n",
    "    orders o\n",
    "JOIN\n",
    "    customers c ON o.customer_id = c.customer_id\n",
    "JOIN\n",
    "    products p ON o.product_id = p.product_id\n",
    "WHERE\n",
    "    o.order_date >= DATE_SUB(CURRENT_DATE, 30)\n",
    "    AND o.status = 'shipped'\n",
    "ORDER BY\n",
    "    o.order_date DESC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "269869d6-b8c4-4710-998f-18fa90d88f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f0711d4-51a6-4d4f-a6f8-0df870459fd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== FUNCTION MAPPING VALIDATION ===\nDirect mappings loaded: 150\nFunctions requiring manual conversion: 30\n\nCritical function mappings:\n  ✓ DATE_SUB        → DATE_SUB       \n  ✓ DATE_TRUNC      → TRUNC          \n  ✓ NVL             → COALESCE       \n  ✗ LISTAGG         → MISSING        \n\n=== CONVERSION REPORT ===\nTotal queries processed: 1\nFully converted queries: 1\nQueries needing manual review: 0\n\n=== FULL CONVERTED OUTPUT ===\n\n-- Query 1 --\nSELECT DISTINCT\n    o.order_id,\n    c.customer_name,\n    p.product_name,\n    o.order_date,\n    CASE \n        WHEN o.total_amount > 500 THEN 'High'\n        WHEN o.total_amount BETWEEN 100 AND 500 THEN 'Medium'\n        ELSE 'Low'\n    END AS order_value_category\nFROM\n    orders o\nJOIN\n    customers c ON o.customer_id = c.customer_id\nJOIN\n    products p ON o.product_id = p.product_id\nWHERE\n    o.order_date >= DATE_SUB(CURRENT_DATE, 30)\n    AND o.status = 'shipped'\nORDER BY\n    o.order_date DESC\n--------------------------------------------------------------------------------\n\nOutput file created at: /Volumes/workspace/default/adhyan/Redshift_conversion_converted.sql\n\nFirst 300 characters of output file:\nSELECT DISTINCT\n    o.order_id,\n    c.customer_name,\n    p.product_name,\n    o.order_date,\n    CASE \n        WHEN o.total_amount > 500 THEN 'High'\n        WHEN o.total_amount BETWEEN 100 AND 500 THEN 'Medium'\n        ELSE 'Low'\n    END AS order_value_category\nFROM\n    orders o\nJOIN\n    customers c O\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Set\n",
    "\n",
    "def clean_func_name(name: str) -> str:\n",
    "    \"\"\"Clean function name by removing parameters and normalizing case.\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return ''\n",
    "    # Remove everything after first parenthesis and clean whitespace\n",
    "    return re.sub(r'\\(.*\\)', '', str(name)).strip().upper()\n",
    "\n",
    "def load_mappings(csv_path: str) -> Tuple[Dict[str, dict], Dict[str, dict], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and validate function mappings from CSV.\n",
    "    Returns:\n",
    "        - function_map: Dictionary of direct replacements\n",
    "        - manual_funcs: Dictionary of functions needing manual conversion\n",
    "        - full_mapping: Complete DataFrame for reference\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mapping_df = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
    "    except UnicodeDecodeError:\n",
    "        mapping_df = pd.read_csv(csv_path, encoding='latin1')\n",
    "    \n",
    "    # Clean column names and fill empty values\n",
    "    mapping_df.columns = [col.strip() for col in mapping_df.columns]\n",
    "    mapping_df.fillna('', inplace=True)\n",
    "    \n",
    "    # Validate critical mappings\n",
    "    required_mappings = {\n",
    "        'DATE_SUB': 'DATE_SUB',\n",
    "        'DATE_TRUNC': 'DATE_TRUNC',\n",
    "        'NVL': 'COALESCE'\n",
    "    }\n",
    "    \n",
    "    # Create direct mapping (functions that can be automatically converted)\n",
    "    direct_map = mapping_df[\n",
    "        mapping_df['need to change'].str.contains(\n",
    "            'direct|no need|can be directly replaced', \n",
    "            case=False, \n",
    "            na=False\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Create manual mapping (functions needing special handling)\n",
    "    manual_map = mapping_df[\n",
    "        mapping_df['need to change'].str.contains(\n",
    "            'manual|needs to be done|needs to pass|needs to replace',\n",
    "            case=False,\n",
    "            na=False\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Build function dictionaries with validation\n",
    "    function_map = {}\n",
    "    for _, row in direct_map.iterrows():\n",
    "        func_name = clean_func_name(row['Redshift_function'])\n",
    "        # Skip empty or invalid mappings\n",
    "        if not func_name or func_name.startswith('EXAMPLE'):\n",
    "            continue\n",
    "        function_map[func_name] = {\n",
    "            'target': clean_func_name(row['Databricks_function']),\n",
    "            'description': row['Description'],\n",
    "            'example': row['Use in redshift']\n",
    "        }\n",
    "    \n",
    "    # Add required mappings if missing\n",
    "    for func, target in required_mappings.items():\n",
    "        if func not in function_map:\n",
    "            function_map[func] = {\n",
    "                'target': target,\n",
    "                'description': f'Automatically added required mapping for {func}',\n",
    "                'example': f'{func}(...)'\n",
    "            }\n",
    "    \n",
    "    manual_funcs = {}\n",
    "    for _, row in manual_map.iterrows():\n",
    "        func_name = clean_func_name(row['Redshift_function'])\n",
    "        if not func_name:\n",
    "            continue\n",
    "        manual_funcs[func_name] = {\n",
    "            'suggestion': row['Databricks_function'],\n",
    "            'description': row['Description'],\n",
    "            'example': row['Use in redshift']\n",
    "        }\n",
    "    \n",
    "    return function_map, manual_funcs, mapping_df\n",
    "\n",
    "def preprocess_sql(sql_content: str) -> List[str]:\n",
    "    \"\"\"Split SQL into individual queries while handling edge cases.\"\"\"\n",
    "    # Remove comments (both single-line and multi-line)\n",
    "    sql_content = re.sub(r'--.*?$|/\\*.*?\\*/', '', sql_content, flags=re.MULTILINE|re.DOTALL)\n",
    "    \n",
    "    # Split on semicolons that aren't within strings\n",
    "    queries = [\n",
    "        q.strip() \n",
    "        for q in re.split(r';(?=(?:[^\\'\"]|\\'[^\\']*\\'|\"[^\"]*\")*$)', sql_content)\n",
    "        if q.strip()\n",
    "    ]\n",
    "    return queries\n",
    "\n",
    "def convert_function_call(match: re.Match, function_map: Dict[str, dict]) -> str:\n",
    "    \"\"\"Convert a single function call using the mapping rules with exact matching.\"\"\"\n",
    "    func_name = match.group(1).upper()  # Force uppercase comparison\n",
    "    params = match.group(2)\n",
    "    \n",
    "    # Get mapping info with exact match\n",
    "    mapping = function_map.get(func_name)\n",
    "    \n",
    "    if mapping:\n",
    "        return f\"{mapping['target']}({params})\"\n",
    "    return match.group(0)  # Return original if no mapping found\n",
    "\n",
    "def convert_query(query: str, function_map: Dict[str, dict], manual_funcs: Dict[str, dict]) -> Tuple[str, List[dict]]:\n",
    "    \"\"\"Convert a single SQL query from Redshift to Databricks with validation.\"\"\"\n",
    "    flagged = []\n",
    "    converted = query\n",
    "    \n",
    "    # First pass: Check for unmapped functions\n",
    "    all_known_funcs = set(function_map.keys()).union(set(manual_funcs.keys()))\n",
    "    found_funcs = set(re.findall(r'\\b([A-Z_][A-Z0-9_]*)\\s*\\(', query, re.IGNORECASE))\n",
    "    \n",
    "    for func in found_funcs:\n",
    "        func_upper = func.upper()\n",
    "        if func_upper not in all_known_funcs:\n",
    "            flagged.append({\n",
    "                'function': func_upper,\n",
    "                'suggestion': 'UNMAPPED_FUNCTION',\n",
    "                'description': 'Function not found in mapping file',\n",
    "                'example': f'{func_upper}(...)'\n",
    "            })\n",
    "    \n",
    "    # Second pass: Convert directly mappable functions\n",
    "    for func_name, mapping in function_map.items():\n",
    "        # Use word boundaries and exact matching\n",
    "        pattern = re.compile(rf'\\b({re.escape(func_name)})\\b\\s*\\((.*?)\\)', re.IGNORECASE)\n",
    "        converted = pattern.sub(\n",
    "            lambda m: convert_function_call(m, function_map),\n",
    "            converted\n",
    "        )\n",
    "    \n",
    "    # Third pass: Flag manual functions\n",
    "    for func_name, details in manual_funcs.items():\n",
    "        if re.search(rf'\\b{re.escape(func_name)}\\b\\s*\\(', converted, re.IGNORECASE):\n",
    "            flagged.append({\n",
    "                'function': func_name,\n",
    "                'suggestion': details['suggestion'],\n",
    "                'description': details['description'],\n",
    "                'example': details['example']\n",
    "            })\n",
    "    \n",
    "    return converted, flagged\n",
    "\n",
    "def process_conversion(input_sql_path: str, output_sql_path: str, mapping_csv_path: str):\n",
    "    \"\"\"Main conversion workflow with enhanced validation.\"\"\"\n",
    "    # Load mappings\n",
    "    function_map, manual_funcs, full_mapping = load_mappings(mapping_csv_path)\n",
    "    \n",
    "    # Diagnostic output\n",
    "    print(\"\\n=== FUNCTION MAPPING VALIDATION ===\")\n",
    "    print(f\"Direct mappings loaded: {len(function_map)}\")\n",
    "    print(f\"Functions requiring manual conversion: {len(manual_funcs)}\")\n",
    "    \n",
    "    # Verify critical mappings\n",
    "    critical_funcs = ['DATE_SUB', 'DATE_TRUNC', 'NVL', 'LISTAGG']\n",
    "    print(\"\\nCritical function mappings:\")\n",
    "    for func in critical_funcs:\n",
    "        status = \"✓\" if func in function_map else \"✗\"\n",
    "        target = function_map.get(func, {}).get('target', 'MISSING')\n",
    "        print(f\"  {status} {func.ljust(15)} → {target.ljust(15)}\")\n",
    "    \n",
    "    # Read and preprocess SQL\n",
    "    with open(input_sql_path, 'r', encoding='utf-8') as f:\n",
    "        sql_content = f.read()\n",
    "    \n",
    "    queries = preprocess_sql(sql_content)\n",
    "    converted_queries = []\n",
    "    manual_reviews = []\n",
    "    \n",
    "    # Process each query\n",
    "    for i, query in enumerate(queries, 1):\n",
    "        converted, flagged = convert_query(query, function_map, manual_funcs)\n",
    "        converted_queries.append(converted)\n",
    "        if flagged:\n",
    "            manual_reviews.append({\n",
    "                'query_number': i,\n",
    "                'converted_query': converted,\n",
    "                'flagged_functions': flagged\n",
    "            })\n",
    "    \n",
    "    # Write output\n",
    "    with open(output_sql_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\\n\".join(converted_queries))\n",
    "    \n",
    "    # Generate comprehensive report\n",
    "    print(\"\\n=== CONVERSION REPORT ===\")\n",
    "    print(f\"Total queries processed: {len(queries)}\")\n",
    "    print(f\"Fully converted queries: {len(queries) - len(manual_reviews)}\")\n",
    "    print(f\"Queries needing manual review: {len(manual_reviews)}\")\n",
    "    \n",
    "    # Display all converted queries\n",
    "    print(\"\\n=== FULL CONVERTED OUTPUT ===\")\n",
    "    for i, query in enumerate(converted_queries, 1):\n",
    "        print(f\"\\n-- Query {i} --\")\n",
    "        print(query)\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Display manual review items\n",
    "    if manual_reviews:\n",
    "        print(\"\\n=== MANUAL REVIEW REQUIRED ===\")\n",
    "        for review in manual_reviews:\n",
    "            print(f\"\\nQuery #{review['query_number']}:\")\n",
    "            print(review['converted_query'])\n",
    "            print(\"\\nFunctions needing attention:\")\n",
    "            for func in review['flagged_functions']:\n",
    "                print(f\"\\n* {func['function']}:\")\n",
    "                print(f\"  Description: {func['description']}\")\n",
    "                print(f\"  Suggestion: {func['suggestion']}\")\n",
    "                print(f\"  Example: {func['example']}\")\n",
    "            print(\"=\" * 80)\n",
    "    \n",
    "    # Verify output file\n",
    "    print(f\"\\nOutput file created at: {output_sql_path}\")\n",
    "    try:\n",
    "        with open(output_sql_path, 'r') as f:\n",
    "            print(\"\\nFirst 300 characters of output file:\")\n",
    "            print(f.read(300))\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError verifying output file: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    CSV_PATH = '/Volumes/workspace/default/adhyan/Redshift and Databricks functions(Sheet1).csv'\n",
    "    INPUT_SQL = '/Volumes/workspace/default/adhyan/Redshift_conversion.sql'\n",
    "    OUTPUT_SQL = '/Volumes/workspace/default/adhyan/Redshift_conversion_converted.sql'\n",
    "    \n",
    "    # Run conversion\n",
    "    process_conversion(INPUT_SQL, OUTPUT_SQL, CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d97bb605-6fa1-4294-9f9c-0f9881d75b95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>customer_name</th><th>product_name</th><th>order_date</th><th>order_value_category</th></tr></thead><tbody><tr><td>104</td><td>Alice Johnson</td><td>Headphones</td><td>2025-06-10</td><td>Medium</td></tr><tr><td>101</td><td>Alice Johnson</td><td>Laptop</td><td>2025-06-01</td><td>High</td></tr><tr><td>102</td><td>Bob Smith</td><td>Headphones</td><td>2025-05-20</td><td>Medium</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         104,
         "Alice Johnson",
         "Headphones",
         "2025-06-10",
         "Medium"
        ],
        [
         101,
         "Alice Johnson",
         "Laptop",
         "2025-06-01",
         "High"
        ],
        [
         102,
         "Bob Smith",
         "Headphones",
         "2025-05-20",
         "Medium"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "order_id",
            "nullable": false,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "customer_name",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "product_name",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "order_date",
            "nullable": true,
            "type": "date"
           },
           {
            "metadata": {},
            "name": "order_value_category",
            "nullable": false,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 28
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "customer_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "order_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "order_value_category",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "SELECT DISTINCT\n",
    "    o.order_id,\n",
    "    c.customer_name,\n",
    "    p.product_name,\n",
    "    o.order_date,\n",
    "    CASE \n",
    "        WHEN o.total_amount > 500 THEN 'High'\n",
    "        WHEN o.total_amount BETWEEN 100 AND 500 THEN 'Medium'\n",
    "        ELSE 'Low'\n",
    "    END AS order_value_category\n",
    "FROM\n",
    "    orders o\n",
    "JOIN\n",
    "    customers c ON o.customer_id = c.customer_id\n",
    "JOIN\n",
    "    products p ON o.product_id = p.product_id\n",
    "WHERE\n",
    "    o.order_date >= DATE_SUB(CURRENT_DATE, 30)\n",
    "    AND o.status = 'shipped'\n",
    "ORDER BY\n",
    "    o.order_date DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f720836b-6390-4cf1-b460-1ae9325f694e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "clear sepration between csv and CSV-driven mappings and critical fallbacks   \n",
    "  # Define critical function fallbacks (only used if missing in CSV)\n",
    "  code is primarily taking mappings from the CSV file, but it also has some automatic fallbacks for critical functions.\n",
    "  It processes two main categories from the CSV:\n",
    "direct_map = mapping_df[mapping_df['need to change'].str.contains('direct|no need|can be directly replaced', ...)]\n",
    "manual_map = mapping_df[mapping_df['need to change'].str.contains('manual|needs to be done|needs to pass|needs to replace', ...)]\n",
    "2. Automatic Fallback Mappings\n",
    "The code includes required_mappings as a safety net for critical functions:\n",
    "\n",
    "python\n",
    "required_mappings = {\n",
    "    'DATE_SUB': 'DATE_SUB',       # Ensures DATE_SUB is always mapped\n",
    "    'DATE_TRUNC': 'DATE_TRUNC',   # Ensures DATE_TRUNC is always mapped\n",
    "    'NVL': 'COALESCE'             # Ensures NVL maps to COALESCE\n",
    "}\n",
    "\n",
    "\n",
    "Clearly reports what's missing (like LISTAGG)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7713434-a028-4d1c-bf9f-a76134cf1420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df8cc365-b679-46a9-8e81-e12f96e00226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MAPPING VALIDATION ===\nDirect mappings: 151\nManual functions: 30\n\nCritical function status:\n  ✓ DATE_SUB   → DATE_SUB       \n  ✓ NVL        → COALESCE       \n  ✓ LISTAGG    → CONCAT_WS      \n  ✓ DATE_TRUNC → TRUNC          \n\n=== CONVERSION PROCESS ===\n\nProcessing Query 1:\nOriginal query:\nWITH user_stats AS (\n  SELECT \n    user_id,\n    COUNT(DISTINCT order_id) AS order_count,\n    SUM(CASE WHEN refunded = FALSE THEN amount ELSE 0 END) AS net_revenue,\n    APPROXIMATE PERCENTILE_DISC(0.5) WITHIN GROUP (ORDER BY amount) AS median_order\n  FROM orders\n  WHERE order_date BETWEEN DATEADD(month, -3, CURRENT_DATE) AND CURRENT_DATE\n  GROUP BY user_id\n)\nSELECT\n  u.user_id,\n  u.username,\n  us.order_count,\n  us.net_revenue,\n  TO_CHAR(us.net_revenue/us.order_count, '999.99') AS avg_order_value,\n  DATE_DIFF('day', u.signup_date, CURRENT_DATE) AS days_since_signup\nFROM users u\nJOIN user_stats us ON u.user_id = us.user_id\nWHERE us.order_count > 1\nORDER BY us.net_revenue DESC\nLIMIT 100\n\nConverted query:\nWITH user_stats AS (\n  SELECT \n    user_id,\n    COUNT(DISTINCT order_id) AS order_count,\n    SUM(CASE WHEN refunded = FALSE THEN amount ELSE 0 END) AS net_revenue,\n    APPROXIMATE PERCENTILE_DISC(0.5) WITHIN GROUP (ORDER BY amount) AS median_order\n  FROM orders\n  WHERE order_date BETWEEN DATEADD(month, -3, CURRENT_DATE) AND CURRENT_DATE\n  GROUP BY user_id\n)\nSELECT\n  u.user_id,\n  u.username,\n  us.order_count,\n  us.net_revenue,\n  TO_CHAR(us.net_revenue/us.order_count, '999.99') AS avg_order_value,\n  DATE_DIFF('day', u.signup_date, CURRENT_DATE) AS days_since_signup\nFROM users u\nJOIN user_stats us ON u.user_id = us.user_id\nWHERE us.order_count > 1\nORDER BY us.net_revenue DESC\nLIMIT 100\n\nFlags raised:\n  - AS: No mapping found in CSV or fallbacks\n    Suggested fix: UNMAPPED_FUNCTION\n\n=== FINAL CONVERSION REPORT ===\nTotal queries processed: 1\nQueries requiring manual review: 1\n\nOutput saved to: /Volumes/workspace/default/adhyan/databricks_converted.sql\n\n=== QUERIES NEEDING MANUAL REVIEW ===\n\nQuery 1:\nWITH user_stats AS (\n  SELECT \n    user_id,\n    COUNT(DISTINCT order_id) AS order_count,\n    SUM(CASE WHEN refunded = FALSE THEN amount ELSE 0 END) AS net_revenue,\n    APPROXIMATE PERCENTILE_DISC(0.5) WITHIN GROUP (ORDER BY amount) AS median_order\n  FROM orders\n  WHERE order_date BETWEEN DATEADD(month, -3, CURRENT_DATE) AND CURRENT_DATE\n  GROUP BY user_id\n)\nSELECT\n  u.user_id,\n  u.username,\n  us.order_count,\n  us.net_revenue,\n  TO_CHAR(us.net_revenue/us.order_count, '999.99') AS avg_order_value,\n  DATE_DIFF('day', u.signup_date, CURRENT_DATE) AS days_since_signup\nFROM users u\nJOIN user_stats us ON u.user_id = us.user_id\nWHERE us.order_count > 1\nORDER BY us.net_revenue DESC\nLIMIT 100\n\nIssues:\n  - AS: No mapping found in CSV or fallbacks\n    Suggested fix: UNMAPPED_FUNCTION\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Set, Optional, Pattern, Match\n",
    "\n",
    "def clean_func_name(name: str) -> str:\n",
    "    \"\"\"Clean function name by removing parameters and normalizing case.\"\"\"\n",
    "    if pd.isna(name) or not isinstance(name, str):\n",
    "        return ''\n",
    "    return re.sub(r'\\(.*\\)', '', name).strip().upper()\n",
    "\n",
    "def load_mappings(csv_path: str) -> Tuple[Dict[str, dict], Dict[str, dict], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load function mappings from CSV with strict validation.\n",
    "    Returns:\n",
    "        - function_map: Verified direct replacements\n",
    "        - manual_funcs: Functions needing manual handling\n",
    "        - full_mapping: Original DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mapping_df = pd.read_csv(csv_path, encoding='utf-8-sig').fillna('')\n",
    "    except (UnicodeDecodeError, FileNotFoundError) as e:\n",
    "        try:\n",
    "            mapping_df = pd.read_csv(csv_path, encoding='latin1').fillna('')\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load CSV: {str(e)}\")\n",
    "    \n",
    "    mapping_df.columns = [col.strip() for col in mapping_df.columns]\n",
    "\n",
    "    # Define critical function fallbacks\n",
    "    CRITICAL_FALLBACKS = {\n",
    "        'DATE_SUB': {'target': 'DATE_SUB', 'description': 'Date subtraction', 'example': 'DATE_SUB(date, days)'},\n",
    "        'NVL': {'target': 'COALESCE', 'description': 'Null value replacement', 'example': 'NVL(expr, default)'},\n",
    "        'LISTAGG': {'target': 'CONCAT_WS', 'description': 'String aggregation', 'example': 'LISTAGG(col, delimiter)'}\n",
    "    }\n",
    "\n",
    "    # Process direct mappings with more robust filtering\n",
    "    direct_map = mapping_df[\n",
    "        mapping_df['need to change'].str.contains(\n",
    "            r'direct|no need|can be directly replaced', \n",
    "            case=False, \n",
    "            na=False, regex=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    function_map = {}\n",
    "    for _, row in direct_map.iterrows():\n",
    "        try:\n",
    "            rs_func = clean_func_name(row['Redshift_function'])\n",
    "            db_func = clean_func_name(row['Databricks_function'])\n",
    "            if rs_func and db_func and not rs_func.startswith(('EXAMPLE', 'TEST')):\n",
    "                function_map[rs_func] = {\n",
    "                    'target': db_func,\n",
    "                    'description': row.get('Description', ''),\n",
    "                    'example': row.get('Use in redshift', '')\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Skipping row due to error - {str(e)}\")\n",
    "\n",
    "    # Add critical fallbacks only if missing\n",
    "    for func, mapping in CRITICAL_FALLBACKS.items():\n",
    "        if func not in function_map:\n",
    "            function_map[func] = mapping\n",
    "\n",
    "    # Process manual mappings with error handling\n",
    "    manual_map = mapping_df[\n",
    "        mapping_df['need to change'].str.contains(\n",
    "            r'manual|needs to be done|needs to pass|needs to replace',\n",
    "            case=False,\n",
    "            na=False, regex=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    manual_funcs = {}\n",
    "    for _, row in manual_map.iterrows():\n",
    "        try:\n",
    "            rs_func = clean_func_name(row['Redshift_function'])\n",
    "            if rs_func:\n",
    "                manual_funcs[rs_func] = {\n",
    "                    'suggestion': clean_func_name(row['Databricks_function']),\n",
    "                    'description': row.get('Description', ''),\n",
    "                    'example': row.get('Use in redshift', '')\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Skipping manual mapping row - {str(e)}\")\n",
    "\n",
    "    return function_map, manual_funcs, mapping_df\n",
    "\n",
    "def preprocess_sql(sql_content: str) -> List[str]:\n",
    "    \"\"\"Split SQL into clean, individual queries.\"\"\"\n",
    "    # Remove all comments more robustly\n",
    "    sql_content = re.sub(r'--[^\\n]*|/\\*.*?\\*/', '', sql_content, flags=re.DOTALL)\n",
    "    # Split on semicolons not in strings with better handling\n",
    "    return [\n",
    "        q.strip() for q in re.split(r';(?=(?:[^\\'\"]|\\'[^\\']*\\'|\"[^\"]*\")*$)', sql_content)\n",
    "        if q.strip()\n",
    "    ]\n",
    "\n",
    "def convert_function_call(match: re.Match, function_map: Dict[str, dict]) -> str:\n",
    "    \"\"\"Convert function call with exact matching and parameter preservation.\"\"\"\n",
    "    func_name = match.group(1).upper()\n",
    "    params = match.group(2)\n",
    "    if func_name in function_map:\n",
    "        return f\"{function_map[func_name]['target']}({params})\"\n",
    "    return match.group(0)\n",
    "\n",
    "def convert_query(query: str, function_map: Dict[str, dict], manual_funcs: Dict[str, dict]) -> Tuple[str, List[dict]]:\n",
    "    \"\"\"Convert query with comprehensive validation.\"\"\"\n",
    "    flagged = []\n",
    "    converted = query\n",
    "    \n",
    "    # More robust function detection\n",
    "    func_pattern: Pattern = re.compile(r'\\b([A-Z_][A-Z0-9_]*)\\s*\\((?![^()]*\\))', re.IGNORECASE)\n",
    "    found_funcs = {f.upper() for f in func_pattern.findall(query)}\n",
    "    \n",
    "    all_mapped = set(function_map.keys()).union(manual_funcs.keys())\n",
    "    \n",
    "    # Check for unmapped functions\n",
    "    for func in found_funcs - all_mapped:\n",
    "        flagged.append({\n",
    "            'function': func,\n",
    "            'suggestion': 'UNMAPPED_FUNCTION',\n",
    "            'description': 'No mapping found in CSV or fallbacks',\n",
    "            'example': f'{func}(...)'\n",
    "        })\n",
    "\n",
    "    # Convert direct mappings\n",
    "    for func_name in found_funcs & set(function_map.keys()):\n",
    "        pattern = re.compile(rf'\\b({re.escape(func_name)})\\b\\s*\\((.*?)\\)', re.IGNORECASE)\n",
    "        converted = pattern.sub(lambda m: convert_function_call(m, function_map), converted)\n",
    "\n",
    "    # Flag manual functions\n",
    "    for func_name in found_funcs & set(manual_funcs.keys()):\n",
    "        flagged.append(manual_funcs[func_name])\n",
    "\n",
    "    return converted, flagged\n",
    "\n",
    "def process_conversion(input_path: str, output_path: str, mapping_path: str) -> None:\n",
    "    \"\"\"End-to-end conversion workflow with error handling.\"\"\"\n",
    "    try:\n",
    "        function_map, manual_funcs, _ = load_mappings(mapping_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading mappings: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Validation report\n",
    "    print(\"=== MAPPING VALIDATION ===\")\n",
    "    print(f\"Direct mappings: {len(function_map)}\")\n",
    "    print(f\"Manual functions: {len(manual_funcs)}\")\n",
    "    print(\"\\nCritical function status:\")\n",
    "    for func in ['DATE_SUB', 'NVL', 'LISTAGG', 'DATE_TRUNC']:\n",
    "        status = \"✓\" if func in function_map else \"✗\"\n",
    "        target = function_map.get(func, {}).get('target', 'MISSING')\n",
    "        print(f\"  {status} {func.ljust(10)} → {target.ljust(15)}\")\n",
    "\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            queries = preprocess_sql(f.read())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading input SQL: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    results = []\n",
    "    manual_reviews = []\n",
    "    \n",
    "    print(\"\\n=== CONVERSION PROCESS ===\")\n",
    "    for i, query in enumerate(queries, 1):\n",
    "        try:\n",
    "            print(f\"\\nProcessing Query {i}:\")\n",
    "            print(\"Original query:\")\n",
    "            print(query)\n",
    "            \n",
    "            converted, flags = convert_query(query, function_map, manual_funcs)\n",
    "            results.append(converted)\n",
    "            \n",
    "            print(\"\\nConverted query:\")\n",
    "            print(converted)\n",
    "            \n",
    "            if flags:\n",
    "                manual_reviews.append({'query_num': i, 'query': converted, 'flags': flags})\n",
    "                print(\"\\nFlags raised:\")\n",
    "                for flag in flags:\n",
    "                    print(f\"  - {flag['function']}: {flag.get('description', '')}\")\n",
    "                    print(f\"    Suggested fix: {flag.get('suggestion', 'Add to CSV')}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting query {i}: {str(e)}\")\n",
    "            results.append(query)  # Keep original if conversion fails\n",
    "\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"\\n\\n\".join(results))\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing output: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Generate final report\n",
    "    print(\"\\n=== FINAL CONVERSION REPORT ===\")\n",
    "    print(f\"Total queries processed: {len(queries)}\")\n",
    "    print(f\"Queries requiring manual review: {len(manual_reviews)}\")\n",
    "    print(f\"\\nOutput saved to: {output_path}\")\n",
    "\n",
    "    if manual_reviews:\n",
    "        print(\"\\n=== QUERIES NEEDING MANUAL REVIEW ===\")\n",
    "        for review in manual_reviews:\n",
    "            print(f\"\\nQuery {review['query_num']}:\")\n",
    "            print(review['query'])\n",
    "            print(\"\\nIssues:\")\n",
    "            for flag in review['flags']:\n",
    "                print(f\"  - {flag['function']}: {flag.get('description', '')}\")\n",
    "                print(f\"    Suggested fix: {flag.get('suggestion', 'Add to CSV')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration with single output path\n",
    "    process_conversion(\n",
    "        input_path='/Volumes/workspace/default/adhyan/Sanjivani.sql',\n",
    "        output_path='/Volumes/workspace/default/adhyan/databricks_converted.sql',\n",
    "        mapping_path='/Volumes/workspace/default/adhyan/Redshift and Databricks functions(Sheet1).csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb0e33ed-aa1a-4349-bbba-385eafdfbed1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "157da109-287f-4575-9091-db28a4de9042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "concern about having to manually maintain fallback functions. You'd like the script to work more like an LLM-based transpiler that can handle conversions more intelligently without predefined mappings. Here's how we can enhance your current approach:\n",
    "\n",
    "Key Improvements\n",
    "Dynamic Function Mapping: Instead of hardcoded fallbacks, implement pattern-based conversions\n",
    "\n",
    "Context-Aware Conversion: Analyze function parameters to make smarter conversions\n",
    "\n",
    "Syntax Tree Parsing: Move beyond regex to proper SQL parsing\n",
    "\n",
    "Learning Mechanism: Cache successful conversions for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507a373c-f682-459a-9012-1a871a065a18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== CONVERSION PROCESS ===\n\n\uD83D\uDD27 Processing Statement 1:\n\uD83D\uDCDC Original:\n-- Sample Redshift query with various functions\nSELECT \n    user_id,\n    NVL(username, 'anonymous') AS safe_username,\n    DATE_TRUNC('month', signup_date) AS signup_month,\n    LISTAGG(product_name, '|') WITHIN GROUP (ORDER BY purchase_date) AS products_ordered,\n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY order_amount) AS median_order,\n    DECODE(status, 1, 'active', 2, 'inactive', 'unknown') AS status_text,\n    DATEADD(day, 30, last_login_date) AS renewal_date\nFROM \n    users u\nJOIN \n    orders o ON u.user_id = o.customer_id\nWHERE \n    signup_date >= DATEADD(year, -1, CURRENT_DATE)\n    AND is_verified = TRUE\nGROUP BY \n    user_id, username, signup_date, status, last_login_date\nHAVING \n    COUNT(*) > 1\nORDER BY \n    signup_month DESC;\n\n\uD83D\uDD04 Converted:\n-- Sample Redshift query with various functions\n SELECT   \n         user_id,\n    NVL(username, 'anonymous') AS safe_username,\n    DATE_TRUNC('month', signup_date) AS signup_month,\n    LISTAGG(product_name, '|') WITHIN   GROUP   (ORDER BY purchase_date) AS products_ordered,\n    PERCENTILE_CONT(0.5) WITHIN   GROUP   (ORDER BY order_amount) AS median_order,\n    DECODE(status, 1, 'active', 2, 'inactive', 'unknown') AS status_text,\n    DATEADD(day, 30, last_login_date) AS renewal_date \n FROM   \n         users u \n JOIN   \n         orders o   ON   u.user_id = o.customer_id \n WHERE \n    signup_date >= DATEADD(year, -1, CURRENT_DATE)\n    AND is_verified = TRUE\n GROUP BY   \n         user_id, username, signup_date, status, last_login_date \n HAVING   \n         COUNT(*) > 1 \n ORDER BY   \n         signup_month DESC ;\n\n=== CONVERSION SUMMARY ===\n\uD83D\uDCCA Total statements processed: 1\n⚠️  Statements needing manual review: 0\n\n\uD83D\uDCBE Output saved to: /Volumes/workspace/default/adhyan/databricks_converted.sql\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Set, Optional, Pattern, Match, Any\n",
    "from collections import defaultdict\n",
    "import sqlparse\n",
    "from sqlparse.sql import Function, Identifier, Token\n",
    "from sqlparse.tokens import Name, Punctuation\n",
    "\n",
    "class SQLTranspiler:\n",
    "    def __init__(self, mapping_path: Optional[str] = None):\n",
    "        self.function_map = defaultdict(dict)\n",
    "        self.manual_funcs = defaultdict(dict)\n",
    "        self.learned_conversions = {}\n",
    "        self.sql_keywords = {\n",
    "            'AS', 'SELECT', 'FROM', 'WHERE', 'GROUP', 'BY', 'ORDER', 'LIMIT',\n",
    "            'WITH', 'JOIN', 'ON', 'AND', 'OR', 'NOT', 'CASE', 'WHEN', 'THEN', 'END',\n",
    "            'BETWEEN', 'TRUE', 'FALSE', 'NULL', 'IS', 'IN', 'LIKE', 'DISTINCT', 'HAVING'\n",
    "        }\n",
    "        \n",
    "        if mapping_path:\n",
    "            self.load_mappings(mapping_path)\n",
    "            \n",
    "        # Initialize with intelligent pattern-based conversions\n",
    "        self.init_pattern_based_conversions()\n",
    "    \n",
    "    def init_pattern_based_conversions(self):\n",
    "        \"\"\"Initialize common conversion patterns that don't need explicit mappings\"\"\"\n",
    "        # Date/time functions\n",
    "        self.add_conversion_pattern(\n",
    "            pattern=r'DATEADD\\s*\\(\\s*(day|days)\\s*,\\s*(.+?)\\s*,\\s*(.+?)\\s*\\)',\n",
    "            replacement=r'DATE_ADD(\\3, \\2)',\n",
    "            description='Convert day-based DATEADD to DATE_ADD'\n",
    "        )\n",
    "        self.add_conversion_pattern(\n",
    "            pattern=r'DATEADD\\s*\\(\\s*(month|months)\\s*,\\s*(.+?)\\s*,\\s*(.+?)\\s*\\)',\n",
    "            replacement=r'DATE_ADD(\\3, \\2*30)',\n",
    "            description='Convert month-based DATEADD to approximate days'\n",
    "        )\n",
    "        \n",
    "        # String functions\n",
    "        self.add_conversion_pattern(\n",
    "            pattern=r'LISTAGG\\s*\\(\\s*(.+?)\\s*,\\s*(.+?)\\s*\\)',\n",
    "            replacement=r'ARRAY_JOIN(COLLECT_LIST(\\1), \\2)',\n",
    "            description='Convert LISTAGG to Databricks array functions'\n",
    "        )\n",
    "        \n",
    "        # Conditional functions\n",
    "        self.add_conversion_pattern(\n",
    "            pattern=r'NVL\\s*\\(\\s*(.+?)\\s*,\\s*(.+?)\\s*\\)',\n",
    "            replacement=r'COALESCE(\\1, \\2)',\n",
    "            description='Convert NVL to COALESCE'\n",
    "        )\n",
    "    \n",
    "    def add_conversion_pattern(self, pattern: str, replacement: str, description: str):\n",
    "        \"\"\"Add a regex-based conversion pattern\"\"\"\n",
    "        self.function_map['PATTERNS'][pattern] = {\n",
    "            'replacement': replacement,\n",
    "            'description': description,\n",
    "            'compiled': re.compile(pattern, re.IGNORECASE)\n",
    "        }\n",
    "    \n",
    "    def load_mappings(self, csv_path: str):\n",
    "        \"\"\"Load function mappings from CSV\"\"\"\n",
    "        try:\n",
    "            mapping_df = pd.read_csv(csv_path, encoding='utf-8-sig').fillna('')\n",
    "        except (UnicodeDecodeError, FileNotFoundError) as e:\n",
    "            try:\n",
    "                mapping_df = pd.read_csv(csv_path, encoding='latin1').fillna('')\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Failed to load CSV: {str(e)}\")\n",
    "        \n",
    "        # Process direct mappings\n",
    "        direct_map = mapping_df[\n",
    "            mapping_df['need to change'].str.contains(\n",
    "                r'direct|no need|can be directly replaced', \n",
    "                case=False, \n",
    "                na=False, regex=True\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        for _, row in direct_map.iterrows():\n",
    "            rs_func = self.clean_func_name(row['Redshift_function'])\n",
    "            db_func = self.clean_func_name(row['Databricks_function'])\n",
    "            if rs_func and db_func and rs_func not in self.sql_keywords:\n",
    "                self.function_map[rs_func] = {\n",
    "                    'target': db_func,\n",
    "                    'description': row.get('Description', ''),\n",
    "                    'example': row.get('Use in redshift', '')\n",
    "                }\n",
    "        \n",
    "        # Process manual mappings\n",
    "        manual_map = mapping_df[\n",
    "            mapping_df['need to change'].str.contains(\n",
    "                r'manual|needs to be done|needs to pass|needs to replace',\n",
    "                case=False,\n",
    "                na=False, regex=True\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        for _, row in manual_map.iterrows():\n",
    "            rs_func = self.clean_func_name(row['Redshift_function'])\n",
    "            if rs_func and rs_func not in self.sql_keywords:\n",
    "                self.manual_funcs[rs_func] = {\n",
    "                    'suggestion': self.clean_func_name(row['Databricks_function']),\n",
    "                    'description': row.get('Description', ''),\n",
    "                    'example': row.get('Use in redshift', '')\n",
    "                }\n",
    "    \n",
    "    def clean_func_name(self, name: str) -> str:\n",
    "        \"\"\"Clean function name by removing parameters and normalizing case.\"\"\"\n",
    "        if pd.isna(name) or not isinstance(name, str):\n",
    "            return ''\n",
    "        return re.sub(r'\\(.*\\)', '', name).strip().upper()\n",
    "    \n",
    "    def parse_sql(self, sql_content: str) -> List[str]:\n",
    "        \"\"\"Parse SQL into statements using sqlparse\"\"\"\n",
    "        statements = sqlparse.parse(sql_content)\n",
    "        return [str(stmt) for stmt in statements if str(stmt).strip()]\n",
    "    \n",
    "    def convert_statement(self, statement: str) -> Tuple[str, List[dict]]:\n",
    "        \"\"\"Convert a single SQL statement using parse tree\"\"\"\n",
    "        parsed = sqlparse.parse(statement)[0]\n",
    "        converted_tokens = []\n",
    "        flagged = []\n",
    "        \n",
    "        for token in parsed.tokens:\n",
    "            if token.is_group and isinstance(token, Function):\n",
    "                # Handle function calls\n",
    "                func_name = self.clean_func_name(token.get_name())\n",
    "                params = token.get_parameters()\n",
    "                \n",
    "                if func_name in self.function_map:\n",
    "                    # Apply direct mapping\n",
    "                    converted = self.apply_function_mapping(func_name, params)\n",
    "                    converted_tokens.append(converted)\n",
    "                elif func_name in self.manual_funcs:\n",
    "                    # Flag for manual review\n",
    "                    flagged.append(self.manual_funcs[func_name])\n",
    "                    converted_tokens.append(str(token))\n",
    "                else:\n",
    "                    # Try pattern-based conversion\n",
    "                    converted, pattern_used = self.try_pattern_conversion(str(token))\n",
    "                    if pattern_used:\n",
    "                        converted_tokens.append(converted)\n",
    "                    else:\n",
    "                        # Unknown function\n",
    "                        flagged.append({\n",
    "                            'function': func_name,\n",
    "                            'suggestion': 'UNMAPPED_FUNCTION',\n",
    "                            'description': 'No mapping found',\n",
    "                            'example': str(token)\n",
    "                        })\n",
    "                        converted_tokens.append(str(token))\n",
    "            else:\n",
    "                converted_tokens.append(str(token))\n",
    "        \n",
    "        return ' '.join(converted_tokens), flagged\n",
    "    \n",
    "    def apply_function_mapping(self, func_name: str, params: str) -> str:\n",
    "        \"\"\"Apply function mapping with parameter handling\"\"\"\n",
    "        target = self.function_map[func_name]['target']\n",
    "        \n",
    "        # Special handling for known function patterns\n",
    "        if func_name == 'DATEADD' and len(params.split(',')) == 3:\n",
    "            unit, value, date = [p.strip() for p in params.split(',', 2)]\n",
    "            if 'month' in unit.lower():\n",
    "                try:\n",
    "                    value = str(int(value) * 30)  # Approximate month→day conversion\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            return f\"{target}({date}, {value})\"\n",
    "        \n",
    "        return f\"{target}({params})\"\n",
    "    \n",
    "    def try_pattern_conversion(self, func_call: str) -> Tuple[str, bool]:\n",
    "        \"\"\"Attempt pattern-based conversion\"\"\"\n",
    "        for pattern, data in self.function_map['PATTERNS'].items():\n",
    "            match = data['compiled'].search(func_call)\n",
    "            if match:\n",
    "                converted = data['compiled'].sub(data['replacement'], func_call)\n",
    "                return converted, True\n",
    "        return func_call, False\n",
    "    \n",
    "    def process_conversion(self, input_path: str, output_path: str) -> None:\n",
    "        \"\"\"End-to-end conversion workflow\"\"\"\n",
    "        try:\n",
    "            with open(input_path, 'r', encoding='utf-8') as f:\n",
    "                statements = self.parse_sql(f.read())\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error reading input SQL: {str(e)}\")\n",
    "            return\n",
    "\n",
    "        results = []\n",
    "        manual_reviews = []\n",
    "        \n",
    "        print(\"\\n=== CONVERSION PROCESS ===\")\n",
    "        for i, stmt in enumerate(statements, 1):\n",
    "            try:\n",
    "                print(f\"\\n\uD83D\uDD27 Processing Statement {i}:\")\n",
    "                print(\"\uD83D\uDCDC Original:\")\n",
    "                print(stmt)\n",
    "                \n",
    "                converted, flags = self.convert_statement(stmt)\n",
    "                results.append(converted)\n",
    "                \n",
    "                print(\"\\n\uD83D\uDD04 Converted:\")\n",
    "                print(converted)\n",
    "                \n",
    "                if flags:\n",
    "                    manual_reviews.append({'stmt_num': i, 'stmt': converted, 'flags': flags})\n",
    "                    print(\"\\n⚠️  Flags raised:\")\n",
    "                    for flag in flags:\n",
    "                        print(f\"  - {flag['function']}: {flag.get('description', '')}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error converting statement {i}: {str(e)}\")\n",
    "                results.append(stmt)\n",
    "\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"\\n\\n\".join(results))\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error writing output: {str(e)}\")\n",
    "            return\n",
    "\n",
    "        # Final report\n",
    "        print(\"\\n=== CONVERSION SUMMARY ===\")\n",
    "        print(f\"\uD83D\uDCCA Total statements processed: {len(statements)}\")\n",
    "        print(f\"⚠️  Statements needing manual review: {len(manual_reviews)}\")\n",
    "        print(f\"\\n\uD83D\uDCBE Output saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transpiler = SQLTranspiler(\n",
    "        mapping_path='/Volumes/workspace/default/adhyan/Redshift and Databricks functions(Sheet1).csv'\n",
    "    )\n",
    "    \n",
    "    transpiler.process_conversion(\n",
    "        input_path='/Volumes/workspace/default/adhyan/transpiler.sql',\n",
    "        output_path='/Volumes/workspace/default/adhyan/databricks_converted.sql'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ec93205-5605-4550-b124-2a4227361e14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "368e590d-ae03-443d-ac7c-f8ed45e1d5f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4802111793718193,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Redshift_to_databricks_Migration_Task",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}